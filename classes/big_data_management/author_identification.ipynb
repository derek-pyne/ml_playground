{"cells":[{"cell_type":"markdown","source":["# Author Identification\n\nInput dataset:\n\n```https://www.kaggle.com/c/spooky-author-identification/data```\n\nThe task is to predict the author of a given sentence given a large corpus of sample sentences."],"metadata":{}},{"cell_type":"markdown","source":["Data was loaded into our cluster using the DataBricks UI. We can now select from it into a DataFrame."],"metadata":{}},{"cell_type":"code","source":["df = spark.sql(\"SELECT text, author FROM pandas_train_csv\")\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Let's first start by making sure our `author` column contains the correct 3 values."],"metadata":{}},{"cell_type":"code","source":["display(df.select('author').distinct())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Our DataFrame now looks ready to feed into our pipeline."],"metadata":{}},{"cell_type":"code","source":["display(df.head(10))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, IDF, CountVectorizer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\nlabelIndexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n\ncountv =  CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize=3000, minDF=2.0)\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\nparamGrid = ParamGridBuilder()\\\n  .addGrid(nb.smoothing, [0.4, 0.8, 1.0, 2.0])\\\n  .addGrid(countv.vocabSize, [1000, 3000, 5000, 700])\\\n  .build()\n  \ncv = CrossValidator(estimator=nb, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=4)\n\npipeline = Pipeline(stages=[labelIndexer, tokenizer, countv, idf, cv])\n\ntrain, test = df.randomSplit([0.8, 0.2])\nmodel = pipeline.fit(train)\npredictions = model.transform(test)\naccuracy = evaluator.evaluate(predictions)\n\nprint \"Accuracy on our test set: %g\" % accuracy"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["cv.getEstimator().extractParamMap()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(predictions.select('text', 'label', 'prediction'))"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"Author Identification","notebookId":894804701504368},"nbformat":4,"nbformat_minor":0}
