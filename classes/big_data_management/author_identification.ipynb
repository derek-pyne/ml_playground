{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Identification\n",
    "\n",
    "Input dataset:\n",
    "\n",
    "```https://www.kaggle.com/c/spooky-author-identification/data```\n",
    "\n",
    "The task is to predict the author of a given sentence given a large corpus of sample sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was loaded into our cluster using the DataBricks UI. We can now select from it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT text, author FROM pandas_train_csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start by making sure our `author` column contains the correct 3 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.select('author').distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DataFrame now looks ready to feed into our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "countv =  CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize=3000, minDF=2.0)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(nb.smoothing, [0.4, 0.8, 1.0, 2.0])\\\n",
    "  .addGrid(countv.vocabSize, [1000, 3000, 5000, 700])\\\n",
    "  .build()\n",
    "  \n",
    "cv = CrossValidator(estimator=nb, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=4)\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, tokenizer, countv, idf, cv])\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2])\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print \"Accuracy on our test set: %g\" % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.getEstimator().extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictions.select('text', 'label', 'prediction'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "name": "Author Identification",
  "notebookId": 894804701504368
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
