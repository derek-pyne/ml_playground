{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To serve as slides: \n",
    "- Open a terminal window \n",
    "- `jupyter nbconvert /full/paht/to/notebook.ipynb --to slides --post serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 5 – Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "\n",
    "- Recurrent Neurons \n",
    "- Basic RNNs in TensorFlow \n",
    "- Training RNNs \n",
    "- Deep RNNs \n",
    "- LSTM Cell \n",
    "- GRU Cell \n",
    "- Natural Language Processing\n",
    "\n",
    "    \n",
    "# Learning Outcomes\n",
    "\n",
    "- Understanding of fundamental concepts underlying RNNs\n",
    "- Main problems RNNs face and solutions to fight them\n",
    "- Comprenhension of the different types of cells: LSTMs and GRUs. \n",
    "- Familiarity with the implementation of RNNs using TensorFlow and Keras. \n",
    "- Exposure to applications where RNNs have shown remarkable performance\n",
    "- Grasp of practical considerations: training time, parameters, vanishing/exploding gradients,\n",
    "- Become familiar with the typical architectures\n",
    "- Practical training and deployment considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- Today, we are going to discuss recurrent neural networks (RNN):\n",
    "    - A class of nets that can predict the future :)\n",
    "- They can analyze time series data such as stock prices, \n",
    "- In autonomous driving systems, they can anticipate car trajectories,\n",
    "- They can work on sequences of arbitrary lengths,\n",
    "- They can take sentences, documents, or audio samples as input, making them extremely useful for natural language processing (NLP) systems.\n",
    "- RNNs can generate sentences, image captions, and much more. \n",
    "- We could ask RNNs to predict which are the most likely next notes in a melody, then randomly pick one of these notes and play it. Then ask the net for the next most likely notes, play it, and repeat the process again and again. (Melody composition) see the Magenta project by Google.\n",
    "\n",
    "\n",
    "We will look at the fundamental concepts underlying RNNs, problem and solutions during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "First, let's make sure this notebook has all the required libraries, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#File containing all definitions and utility functions.\n",
    "from setups import *\n",
    "from plotting import *\n",
    "%matplotlib inline\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neurons\n",
    "\n",
    "- A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backwards. \n",
    "\n",
    "- Let’s look at the simplest possible RNN, one cell with feedback: \n",
    "    - At each time step t, this recurrent neuron receives the inputs $x(t)$ as well as its own output from the previous time step, $y(t–1)$. \n",
    "    - We can represent this tiny network against the time axis. This is called unrolling the network through time.\n",
    "\n",
    "\n",
    "![rnn_cell.png](imgs/rnn_cell.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Now, to create a layer:\n",
    "- Every neuron receives both the input vector $x(t)$ and the output vector from the previous time step $y(t–1)$. \n",
    "- Note that both the inputs and outputs are vectors now \n",
    "\n",
    "![rnn_layer.png](imgs/rnn_layer.png)\n",
    "\n",
    "Each recurrent neuron has two sets of weights: \n",
    "- for the inputs $x(t)$: $w_x$\n",
    "- for the outputs of the previous time step, $y(t–1)$: $w_y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can place all the weight vectors for all neurons in two weight matrices, $W_x$ and $W_y$. \n",
    "- The output vector of the whole recurrent layer can then be computed as:  \n",
    "($b$ is the bias vector and $\\phi(.)$ is the activation function).\n",
    "\n",
    "\n",
    "![rnneq1.png](imgs/rnneq1.png)\n",
    "\n",
    "We can compute a recurrent layer’s output in one shot for a whole mini-batch :\n",
    "\n",
    "\n",
    "![rnn_outputeq.png](imgs/rnn_outputeq.png)\n",
    "\n",
    "\n",
    "- $Y(t)$ is an $m \\times n_{neurons}$ matrix containing the layer’s outputs at time step $t$ for each instance in the mini-batch (m is the number of instances in the mini-batch).\n",
    "\n",
    "- $X(t)$ is an $m \\times n_{inputs}$ matrix containing the inputs for all instances \n",
    "\n",
    "- $W_x$ is an $n_{inputs} \\times n_{neurons}$ matrix containing the connection weights of the current step.\n",
    "\n",
    "- $W_y$ is an $n_{neurons} × n_{neurons}$ matrix containing the connection weights for the outputs of the previous time step.\n",
    "\n",
    "- $b$ is a vector of size $n_{neurons}$ containing each neuron’s bias term.\n",
    "\n",
    "The weight matrices $W_x$ and $W_y$ are often concatenated vertically into a single weight matrix $W$\n",
    "\n",
    "The notation $[X(t) Y(t–1)]$ represents the horizontal concatenation of the matrices $X(t)$ and $Y(t–1)$.\n",
    "\n",
    "Notice that Y(t) is a function of X(t) and Y(t–1), which is a function of X(t–1) and Y(t–2), which is a function of X(t–2) and Y(t–3), and so on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Memory Cells\n",
    "\n",
    "- The output of a recurrent neuron at time $t$ is a function of all the inputs from previous times:\n",
    "    - You could say it has a form of memory. \n",
    "    \n",
    "    \n",
    "- A single recurrent neuron, is a very form of memory cell.\n",
    "\n",
    "- In general, a cell’s state $h(t)$ is a function of some inputs $x(t)$ and its state at the previous time step: \n",
    "    - $h(t) = f(h(t–1), x(t))$\n",
    "    \n",
    "    \n",
    "- The network's output $y(t)$, is also a function of the previous $y(t-1)$ and $x(t)$. \n",
    "\n",
    "- We will look at more complex memory cells later today\n",
    "\n",
    "![mem_cells.png](imgs/mem_cells.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Input and Output Sequences\n",
    "\n",
    "- An RNN can simultaneously take a sequence of inputs and produce a sequence of outputs:\n",
    "    - Useful for predicting time series\n",
    "\n",
    "\n",
    "- RNNs can also take a sequence of inputs, and ignore all outputs except for the last one:\n",
    "    - Sequence-to-vector network. i.e. feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score (e.g., from –1 [hate] to +1 [love]).\n",
    "\n",
    "\n",
    "- Single input at the first time step (and zeros for all other time steps), and let it output a sequence \n",
    "    - Vector-to-sequence network. i.e. the input could be an image, and the output could be a caption for that image.\n",
    "\n",
    "\n",
    "- Sequence-to-vector network (encoder), followed by a vector-to-sequence network (decoder).  \n",
    "    - translating a sentence from one language to another. This two-step model, called an Encoder–Decoder. \n",
    "    - This model works better because the last words of a sentence can affect the first words of the translation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![input_output_seq.png](imgs/input_output_seq.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNNs in TensorFlow\n",
    "\n",
    "\n",
    "![rnn_cell.png](imgs/rnn_cell.png)\n",
    "\n",
    "\n",
    "First, We will create an RNN composed of a layer of five recurrent neurons using the tanh activation function. We will assume that the RNN runs over only two time steps, taking input vectors of size 3 at each time step. The following code builds this RNN, unrolled through two time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons],dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This network looks much like a two-layer feedforward neural network, with a few twists: \n",
    "    - The same weights and bias terms are shared by both layers\n",
    "    - We feed inputs at each layer, and we get outputs from each layer. \n",
    "\n",
    "To run the model, we need to feed it the inputs at both time steps, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mini-batch:        instance 0,instance 1,instance 2,instance 3\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This mini-batch contains four instances, each with an input sequence composed of exactly two inputs. At the end, Y0_val and Y1_val contain the outputs of the network at both time steps for all neurons and all instances in the mini-batch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    ">>> print(Y0_val)  # output at t = 0\n",
    "[[-0.0664006   0.96257669  0.68105787  0.70918542 -0.89821595]  # instance 0\n",
    " [ 0.9977755  -0.71978885 -0.99657625  0.9673925  -0.99989718]  # instance 1\n",
    " [ 0.99999774 -0.99898815 -0.99999893  0.99677622 -0.99999988]  # instance 2\n",
    " [ 1.         -1.         -1.         -0.99818915  0.99950868]] # instance 3\n",
    ">>> print(Y1_val)  # output at t = 1\n",
    "[[ 1.         -1.         -1.          0.40200216 -1.        ]  # instance 0\n",
    " [-0.12210433  0.62805319  0.96718419 -0.99371207 -0.25839335]  # instance 1\n",
    " [ 0.99999827 -0.9999994  -0.9999975  -0.85943311 -0.9999879 ]  # instance 2\n",
    " [ 0.99928284 -0.99999815 -0.99990582  0.98579615 -0.92205751]] # instance 3\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Static Unrolling Through Time (TF)\n",
    "The static_rnn() function creates an unrolled RNN network by chaining cells. The following code creates the exact same model as the previous one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1],\n",
    "                                                dtype=tf.float32)\n",
    "Y0, Y1 = output_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Steps:\n",
    "    - Create the input placeholders \n",
    "    - Create a `BasicRNNCell`\n",
    "    - Call static_rnn()\n",
    "    \n",
    "    \n",
    "- The `static_rnn()` function calls the cell factory’s `__call__()` function once per input, creating two copies of the cell, with shared weights and bias terms, and it chains. The `static_rnn()` function returns two objects:\n",
    "    - A python list containing the output tensors for each time step. \n",
    "    - A tensor containing the final states of the network. \n",
    "\n",
    "If there were 50 time steps: You would to have to define 50 input placeholders and 50 output tensors. And, at execution time you would have to feed each of the 50 placeholders and manipulate the 50 outputs. \n",
    "\n",
    "The following code builds the same RNN again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, X_seqs,\n",
    "                                                dtype=tf.float32)\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Now we can run the network by feeding it a single tensor that contains all the mini-batch sequences:\n",
    "\n",
    "X_batch = np.array([\n",
    "         # t = 0     t = 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 1\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 3\n",
    "    ])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "#And we get a single outputs_val tensor for all instances, all time steps, and all neurons:\n",
    "\n",
    ">>> print(outputs_val)\n",
    "[[[-0.91279727  0.83698678 -0.89277941  0.80308062 -0.5283336 ]\n",
    "  [-1.          1.         -0.99794829  0.99985468 -0.99273592]]\n",
    "\n",
    " [[-0.99994391  0.99951613 -0.9946925   0.99030769 -0.94413054]\n",
    "  [ 0.48733309  0.93389565 -0.31362072  0.88573611  0.2424476 ]]\n",
    "\n",
    " [[-1.          0.99999875 -0.99975014  0.99956584 -0.99466234]\n",
    "  [-0.99994856  0.99999434 -0.96058172  0.99784708 -0.9099462 ]]\n",
    "\n",
    " [[-0.95972425  0.99951482  0.96938795 -0.969908   -0.67668229]\n",
    "  [-0.84596014  0.96288228  0.96856463 -0.14777924 -0.9119423 ]]]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, this approach still builds a graph containing one cell per time step. \n",
    "    - If there were 50 time steps, the graph would look pretty ugly. \n",
    "\n",
    "Fortunately, there is a better solution: the `dynamic_rnn()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dynamic Unrolling Through Time\n",
    "\n",
    "The `dynamic_rnn()` function uses a `while_loop()` operation to run over the cell the appropriate number of times, and you can set `swap_memory=True` if you want it to swap the GPU’s memory to the CPU’s memory during backpropagation to avoid OOM errors.\n",
    "\n",
    "- It accepts a single tensor for all inputs at every time step and outputs a single tensor for all outputs at every time step \n",
    "- There is no need to stack, unstack, or transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "#During backpropagation, the while_loop() operation does the appropriate magic: \n",
    "#it stores the tensor values for each iteration during the forward pass \n",
    "#so it can use them to compute gradients during the reverse pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Handling Variable Length Input Sequences\n",
    "\n",
    "- What if the input sequences have variable lengths (e.g., like sentences)? \n",
    "    - In this case you should set the sequence_length argument when calling the dynamic_rnn() (or static_rnn()) function; \n",
    "    - It must be a 1D tensor indicating the length of the input sequence for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "[...]\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, suppose the second input sequence contains only one input instead of two. It must be padded with a zero vector in order to fit in the input tensor X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 1 (padded with a zero vector)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 3\n",
    "    ])\n",
    "seq_length_batch = np.array([2, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now need to feed values for both placeholders X and seq_length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, the RNN outputs zero vectors for every time step past the input sequence length:\n",
    "\n",
    "```\n",
    ">>> print(outputs_val)\n",
    "[[[-0.68579948 -0.25901747 -0.80249101 -0.18141513 -0.37491536]\n",
    "  [-0.99996698 -0.94501185  0.98072106 -0.9689762   0.99966913]]  # final state\n",
    "\n",
    " [[-0.99099374 -0.64768541 -0.67801034 -0.7415446   0.7719509 ]   # final state\n",
    "  [ 0.          0.          0.          0.          0.        ]]  # zero vector\n",
    "\n",
    " [[-0.99978048 -0.85583007 -0.49696958 -0.93838578  0.98505187]\n",
    "  [-0.99951065 -0.89148796  0.94170523 -0.38407657  0.97499216]]  # final state\n",
    "\n",
    " [[-0.02052618 -0.94588047  0.99935204  0.37283331  0.9998163 ]\n",
    "  [-0.91052347  0.05769409  0.47446665 -0.44611037  0.89394671]]] # final state\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The states tensor contains the final state of each cell (excluding the zero vectors):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    ">>> print(states_val)\n",
    "[[-0.99996698 -0.94501185  0.98072106 -0.9689762   0.99966913]  # t = 1\n",
    " [-0.99099374 -0.64768541 -0.67801034 -0.7415446   0.7719509 ]  # t = 0 !!!\n",
    " [-0.99951065 -0.89148796  0.94170523 -0.38407657  0.97499216]  # t = 1\n",
    " [-0.91052347  0.05769409  0.47446665 -0.44611037  0.89394671]] # t = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Handling Variable-Length Output Sequences\n",
    "\n",
    "- i.e. the length of a translated sentence is generally different from the length of the input sentence. \n",
    "    - In this case, the most common solution is to define a special output called an end-of-sequence token (EOS token). Any output past the EOS should be ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training RNNs\n",
    "\n",
    "The approach is to unroll the RNN through time and then use regular backpropagation  --backpropagation through time (BPTT).\n",
    "\n",
    "\n",
    "![training_rnns.png](imgs/training_rnns.png)\n",
    "\n",
    "\n",
    "- There is a first forward pass through the unrolled network ; \n",
    "- then the output sequence is evaluated using a cost function \n",
    "- and the gradients of that cost function are propagated backward through the unrolled network \n",
    "- finally the model parameters are updated using the gradients computed during BPTT. \n",
    "\n",
    "Note that the gradients flow backward through all the outputs used by the cost function, not just through the final output. \n",
    "\n",
    "Since the same parameters W and b are used at each time step, backpropagation will do the right thing and sum over all time steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a Sequence Classifier\n",
    "\n",
    "- Let’s train an RNN to classify MNIST images:\n",
    "    - We will treat each image as a sequence of 28 rows of 28 pixels each \n",
    "    - We will use cells of 150 recurrent neurons, plus a fully connected layer containing 10 neurons connected to the output of the last time step, followed by a softmax layer\n",
    "\n",
    "![seq_clf.png](imgs/seq_clf.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Now let’s load the MNIST data and reshape the test data \n",
    "# to [batch_size, n_steps, n_inputs] as is expected by the network. \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#The execution phase is exactly the same as for a Fully Connected network (See Geron ch. 10)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output should look like this:\n",
    "```\n",
    "0 Train accuracy: 0.94 Test accuracy: 0.9308\n",
    "1 Train accuracy: 0.933333 Test accuracy: 0.9431\n",
    "[...]\n",
    "98 Train accuracy: 0.98 Test accuracy: 0.9794\n",
    "99 Train accuracy: 1.0 Test accuracy: 0.9804\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We get over 98% accuracy! Plus you would certainly get a better result by tuning the hyperparameters, initializing the RNN weights using He initialization, training longer, or adding regularization.\n",
    "\n",
    "You can specify an initializer for the RNN by wrapping its construction code in a variable scope (e.g., use `variable_scope(\"rnn\", initializer=variance_scaling_initializer()`) to use He initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training to Predict Time Series\n",
    "Now let’s take a look at how to handle time series, such as stock prices, air temperature, brain wave patterns, and so on. \n",
    "\n",
    "- We will train an RNN to predict the next value in a generated time series: \n",
    "    - Each training instance is a randomly selected sequence of 20 consecutive values from the time series\n",
    "    - The target sequence is the same as the input sequence, except it is shifted by one time step into the future \n",
    "\n",
    "![predict_ts.png](imgs/predict_ts.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let’s create the RNN. It will contain 100 recurrent neurons and we will unroll it over 20 time steps since each training instance will be 20 inputs long. Each input will contain only one feature (the value at that time). The targets are also sequences of 20 inputs, each containing a single value. The code is almost the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general you would have more than just one input feature. For example, if you were trying to predict stock prices, you would likely have many other input features at each time step, such as prices of competing stocks, ratings from analysts, or any other feature that might help the system make its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- At each time step we have an output vector of size 100. But what we actually want is a single output value at each time step.\n",
    "    - The simplest solution is to wrap the cell in an `OutputProjectionWrapper`. A cell wrapper acts like a normal cell, proxying every method call to an underlying cell, but it also adds some functionality. \n",
    "    - The `OutputProjectionWrapper` adds a fully connected layer of linear neurons on top of each output.\n",
    "    - The resulting RNN is represented as:\n",
    "\n",
    "\n",
    "![output_projections.png](imgs/output_projections.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s tweak the preceding code by wrapping the BasicRNNCell into an OutputProjectionWrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "    output_size=n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we need to define the cost function (we could use the MSE), an optimizer (Adam), the training op, and the variable initialization op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#Now on to the execution phase:\n",
    "\n",
    "n_iterations = 1500\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = [...]  # fetch the next training batch\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The program’s output should look like this:\n",
    "\n",
    "```0       MSE: 13.6543\n",
    "100     MSE: 0.538476\n",
    "200     MSE: 0.168532\n",
    "300     MSE: 0.0879579\n",
    "400     MSE: 0.0633425\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once the model is trained, you can make predictions:\n",
    "\n",
    "```\n",
    "X_new = [...]  # New sequences\n",
    "y_pred = sess.run(outputs, feed_dict={X: X_new})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predition after 1000 iterations:\n",
    "\n",
    "![ts_predictions.png](imgs/ts_prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Besides `OutputProjectionWrapper` there is a trickier but more efficient solution: \n",
    "    - Reshape the RNN outputs from `[batch_size, n_steps, n_neurons]` to `[batch_size * n_steps, n_neurons]`, then\n",
    "    - apply a single fully connected layer with the appropriate output size,\n",
    "    - The result is an output tensor of shape `[batch_size * n_steps, n_outputs]`, and then\n",
    "    - reshape the output tensor to `[batch_size, n_steps, n_outputs]`\n",
    "\n",
    "\n",
    "\n",
    "![stack_outputs.png](imgs/stack_outputs.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To implement this solution, we first revert to a basic cell, without the `OutputProjectionWrapper:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then we stack all the outputs using the reshape() operation, apply the fully connected linear layer, and finally unstack all the outputs, again using reshape():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creative RNN\n",
    "\n",
    "- We can use the same model we trained to generate some creative sequences:\n",
    "    - We need to provide a sequence seed containing n_steps values (e.g., full of zeros), \n",
    "    - use the model to predict the next value, \n",
    "    - append this predicted value to the sequence, \n",
    "    - feed the last n_steps values to the model to predict the next value, and so on. \n",
    "    \n",
    "- This process generates a sequence that resemblances the original time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sequence = [0.] * n_steps\n",
    "for iteration in range(300):\n",
    "    X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1)\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "    sequence.append(y_pred[0, -1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![creative_seq.png](imgs/creative_seq.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if you can feed all your favourite music to an RNN and see if it can generate the next hit. You would likely need a much more powerful RNN, with more neurons, and also much deeper. Let’s look at deep RNNs now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep RNNs\n",
    "\n",
    "It is quite common to stack multiple layers of cells:\n",
    "\n",
    "![deeprnn.png](imgs/deeprnn.png)\n",
    "\n",
    "\n",
    "In TensorFlow, you can create several cells and stack them into a MultiRNNCell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#In the following code we stack three identical cells \n",
    "# (but you could very well use various kinds of cells with a different number of neurons:\n",
    "\n",
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                      activation=tf.nn.relu)\n",
    "          for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- The states variable is a tuple containing one tensor per layer, each representing the final state of that layer’s cell (with shape [batch_size, n_neurons]). \n",
    "\n",
    "- If you set `state_is_tuple=False`, then states becomes a single tensor containing the states from every layer, concatenated along the column axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributing a Deep RNN Across Multiple GPUs\n",
    "\n",
    "You can efficiently distribute deep RNNs across multiple GPUs by pinning each layer to a different GPU. However, if you try to create each cell in a different device() block, it will not work:\n",
    "\n",
    "![rnn_cell.png](imgs/rnn_cell.png)\n",
    "\n",
    "\n",
    "- It would fail because a BasicRNNCell is a cell factory, not a cell per se; \n",
    "    - No cells get created when you create the factory, and thus no variables do either. The device block is simply ignored. The cells actually get created later. \n",
    "    - When you call `dynamic_rnn()`, it calls the `MultiRNNCell`, which calls each individual `BasicRNNCell`, which create the actual cells (including their variables). \n",
    "    - Unfortunately, none of these classes provide any way to control the devices on which the variables get created. \n",
    "    - If you try to put the dynamic_rnn() call within a device block, the whole RNN gets pinned to a single device. \n",
    "- The trick is to create your own cell wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):  # BAD! This is ignored.\n",
    "    layer1 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "\n",
    "with tf.device(\"/gpu:1\"):  # BAD! Ignored again.\n",
    "    layer2 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DeviceCellWrapper(tf.contrib.rnn.RNNCell):\n",
    "  def __init__(self, device, cell):\n",
    "    self._cell = cell\n",
    "    self._device = device\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with tf.device(self._device):\n",
    "        return self._cell(inputs, state, scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying Dropout\n",
    "\n",
    "- If you build a very deep RNN, it may end up overfitting the training set. \n",
    "    - To prevent that, a common technique is to apply dropout\n",
    "- You can simply add a dropout layer before or after the RNN as usual, but if you also want to apply dropout between the RNN layers, you need to use a DropoutWrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "cells = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "         for layer in range(n_layers)]\n",
    "cells_drop = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "              for cell in cells]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells_drop)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "# The rest of the construction phase is just like earlier.\n",
    "\n",
    "#During training, you can feed any value you want to the keep_prob placeholder (typically, 0.5):\n",
    "\n",
    "n_iterations = 1500\n",
    "batch_size = 50\n",
    "train_keep_prob = 0.5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        _, mse = sess.run([training_op, loss],\n",
    "                          feed_dict={X: X_batch, y: y_batch,\n",
    "                                     keep_prob: train_keep_prob})\n",
    "    saver.save(sess, \"./my_dropout_time_series_model\")\n",
    "    \n",
    "    \n",
    "#During testing, you should let keep_prob default to 1.0, \n",
    "#effectively turning dropout off (remember that it should only be active during training):\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_dropout_time_series_model\")\n",
    "\n",
    "    X_new = [...] # some test data\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that it is also possible to apply dropout to the outputs by setting output_keep_prob. Tt is also possible to apply dropout to the cell’s state using `state_keep_prob`.\n",
    "\n",
    "Unfortunately, if you want to train an RNN on long sequences, things will get a bit harder. Let’s see why and what you can do about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Difficulty of Training over Many Time Steps\n",
    "\n",
    "- To train an RNN on long sequences, you will need to run it over many time steps, making the unrolled RNN a very deep network. \n",
    "    - Just like any deep neural network it may suffer from the vanishing/exploding gradients problem and take long to train. \n",
    "\n",
    "\n",
    "- You could then do:\n",
    "    - Good parameter initialization, \n",
    "    - nonsaturating activation functions (e.g., ReLU), \n",
    "    - Batch Normalization, \n",
    "    - Gradient Clipping, and \n",
    "    - faster optimizers. \n",
    "    - Event after all. The training will still be very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- One solution is to unroll the RNN only over a limited number of time steps during training (truncated backpropagation through time). \n",
    "    - It can be implemented by truncating the input sequences:\n",
    "        - Reduce `n_steps` during training. In this case, the model will not be able to learn long-term patterns. \n",
    "        - One workaround could be to make sure that these shortened sequences contain both old and recent data, so that the model can learn to use both.\n",
    "            - what if fine-grained data from last year is actually useful? What if there was a brief but significant event that absolutely must be taken into account, even years later?\n",
    "\n",
    "\n",
    "- For RNNs, the memory of the first inputs gradually fades away. Some information is lost after each time step. \n",
    "- After a while, the RNN’s state contains virtually no trace of the first inputs.\n",
    "    - For example, say you want to perform sentiment analysis on a long review that starts with the four words “I loved this movie,” but the rest of the review lists the many things that could have made the movie even better. \n",
    "    - If the RNN gradually forgets the first four words, it will completely misinterpret the review. \n",
    "    - To solve this problem, various types of cells with long-term memory have been introduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM Cell\n",
    "\n",
    "- The Long Short-Term Memory (LSTM) cell was proposed in 1973 by Sepp Hochreiter and Jürgen Schmidhuber.\n",
    "\n",
    "- An LSTM cell will:\n",
    "    - Converge faster while training\n",
    "    - Detect long-term dependencies in the data. \n",
    "    - In TensorFlow, replace `BasicRNNCell` with `BasicLSTMCell`\n",
    "        - `lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)`\n",
    "\n",
    "\n",
    "\n",
    "- LSTM cells manage two state vectors, and for performance reasons they are kept separate by default. You can change this default behavior by setting `state_is_tuple=False`.\n",
    "\n",
    "\n",
    "- Architecture:\n",
    "\n",
    "![lstmcell.png](imgs/lstmcell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it\n",
    "\n",
    "\n",
    "- The LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: \n",
    "    - $h_{(t)}$ the short-term state and \n",
    "    - $c_{(t)}$ as the long-term state.\n",
    "\n",
    "\n",
    "- As the long-term state $c_{(t–1)}$ traverses the network:\n",
    "    - it first goes through a forget gate, dropping some memories, then \n",
    "    - adds some new memories via the addition operation.\n",
    "    - At each time step, some memories are dropped and some memories are added. \n",
    "    \n",
    "    \n",
    "- The long-term state is then copied and passed through the `tanh` function, the result is filtered by the output gate. This produces the short-term state $h_{(t)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here is how it works. The current input vector $x_{(t)}$ and the previous short-term state $h_{(t-1)}$ are fed to four different fully connected layers. They all serve a different purpose:\n",
    "    - The main layer is the one that outputs $g_{(t)}$. It has the usual role of analyzing the current inputs $x_{(t)}$ and the previous $h_{(t-1)}$. \n",
    "        - In a basic cell, there is nothing else than this layer\n",
    "        - In an LSTM cell this layer’s output is partially stored in the long-term state.\n",
    "    - The three other layers are gate controllers. Their outputs range from 0 to 1. \n",
    "        - Their outputs are fed to element-wise multiplication operations, so if they output 0s, they close the gate, and if they output 1s, they open it. Specifically:\n",
    "            - The forget gate controls which parts of the long-term state should be erased.\n",
    "            - The input gate controls which parts of $g_{(t)}$ should be added to the long-term state \n",
    "            - The output gate controls which parts of the long-term state should be read and output at this time step.\n",
    "\n",
    "\n",
    "\n",
    "- An LSTM cell can learn to recognize an important input, store it in the long-term state, learn to preserve it for as long as it is needed, and learn to extract it whenever it is needed. \n",
    "\n",
    "- They have been amazingly successful at capturing long-term patterns in time series, long texts, audio recordings, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnneq.png](imgs/rnneq.png)\n",
    "\n",
    "$W_{xi}$, $W_{xf}$, $W_{xo}$, $W_{xg}$ are the weight matrices of each of the four layers for their connection to the input vector $x_{(t)}$.\n",
    "\n",
    "$W_{hi}$, $W_{hf}$, $W_{ho}$, and $W_{hg}$ are the weight matrices of each of the four layers for their connection to the previous short-term state $h_{(t–1)}$.\n",
    "\n",
    "$b_i$, $b_f$, $b_o$, and $b_g$ are the bias terms for each of the four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Peephole Connections\n",
    "\n",
    "- It may be a good idea to give an RNN a bit more context by letting them peek at the long-term state. \n",
    "    - An LSTM variant with peephole connections was proposed by Felix Gers and Jürgen Schmidhuber in 2006.\n",
    "        - The previous long-term state $c_{(t–1)}$ is added as an input to the controllers of the forget gate and the input gate, and the current long-term state $c_{(t)}$ is added as input to the controller of the output gate.\n",
    "\n",
    "\n",
    "\n",
    "- In TensorFlow, use the LSTMCell and set `use_peepholes=True`\n",
    "    - `lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GRU Cell\n",
    "\n",
    "\n",
    "- The Gated Recurrent Unit (GRU) cell was proposed by Kyunghyun Cho et al. in a 2014\n",
    "\n",
    "![gru_cell.png](imgs/gru_cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GRU Cell\n",
    "\n",
    "- The GRU cell is a simplified version of the LSTM cell, and it seems to perform just as well:\n",
    "    - Both state vectors are merged into a single vector $h_{(t)}$.\n",
    "    - A single gate controller controls both the forget gate and the input gate:\n",
    "        - If the gate controller outputs a 1, the forget gate is open and the input gate is closed. \n",
    "        - If it outputs a 0, the opposite happens. \n",
    "        - Whenever a memory must be stored, the location is erased first. \n",
    "        \n",
    "    - There is no output gate; \n",
    "    - The full state vector is output at every time step. \n",
    "    - There is a new gate controller that controls which part of the previous state will be shown to the main layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Equations\n",
    "\n",
    "![gru_eq.png](imgs/gru_eq.png)\n",
    "\n",
    "- In Tensorflow:\n",
    "    `gru_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)`\n",
    "- LSTM or GRU cells are one of the main reasons behind the success of RNNs in recent years, in particular for applications in natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "\n",
    "- Most of the state-of-the-art NLP applications, such as machine translation, automatic summarization, parsing, sentiment analysis, and more, are based on RNNs.\n",
    "\n",
    "- This topic is very well covered by TensorFlow’s `Word2Vec` and `Seq2Seq` tutorials, so you should definitely check them out.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "- We need to choose a word representation. One option could be to represent each word using a one-hot vector. \n",
    "    - However, with a large vocabulary, this sparse representation would not be efficient at all.\n",
    "    \n",
    "    \n",
    "- Ideally, you want similar words to have similar representations \n",
    "    - For example, In “I drink milk”, the model knows that “milk” is close to “water” but far from “shoes,” then it will know that “I drink water” is probably a valid sentence as well, while “I drink shoes” is probably not. \n",
    "    \n",
    "    \n",
    "- The most common solution is to represent each word in the vocabulary using a small and dense vector (e.g., 150 dimensions), called an embedding, and just let the neural network learn a good embedding for each word during training.\n",
    "\n",
    "\n",
    "- During training, backpropagation automatically moves the embeddings around in a way that helps the neural network perform its task. Typically this means that similar words will gradually cluster close to one another, and even end up organized in a rather meaningful way.\n",
    "    - For example, embeddings may end up placed along various axes that represent gender, singular/plural, adjective/noun, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# you first need to create the variable representing the embeddings \n",
    "# for every word in your vocabulary (initialized randomly)\n",
    "\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 150\n",
    "\n",
    "init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "embeddings = tf.Variable(init_embeds)\n",
    "\n",
    "# Once you have a list of known words. you can feed the word identifiers to TensorFlow\n",
    "# using a placeholder, and apply the embedding_lookup() function to get \n",
    "# the corresponding embeddings:\n",
    "\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])  # from ids...\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)  # ...to embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once your model has learned good word embeddings, they can actually be reused fairly efficiently in any NLP application\n",
    "    - In fact, instead of training your own word embeddings, you may want to download pretrained word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Encoder–Decoder Network for Machine Translation\n",
    "\n",
    "\n",
    "- Let’s take a look at a machine translation model from English to French\n",
    "\n",
    "![translation.png](imgs/translation.png)\n",
    "\n",
    "\n",
    "- The English sentences are fed to the encoder, and the decoder outputs the French translations. \n",
    "    - The decoder is given as input the word that it should have output at the previous step \n",
    "    - For the very first word, it is given a token that represents the beginning of the sentence. \n",
    "    - The decoder is expected to end the sentence with an end-of-sequence (EOS) token (e.g., `<eos>`).\n",
    "\n",
    "- English sentences are reversed. For example “I drink milk” is reversed to “milk drink I.” \n",
    "    - This ensures that the beginning of the English sentence will be fed last to the encoder, since it is the first thing the decoder needs to translate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# An Encoder–Decoder Network for Machine Translation\n",
    "\n",
    "- Each word is initially represented by a simple integer identifier. \n",
    "- Next, an embedding lookup returns the word embedding \n",
    "- These word embeddings are what is actually fed to the encoder and the decoder.\n",
    "- The decoder outputs a score for each word in the output vocabulary,\n",
    "- then the Softmax layer turns these scores into probabilities. \n",
    "    - For example, at the first step the word \"Je\" may have a probability of 20%, \"Tu\" may have a probability of 1%, and so on. The word with the highest probability is output. \n",
    "\n",
    "![word_seq.png](imgs/word_seq.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow sequence-to-sequence\n",
    "\n",
    "- If you go through TensorFlow’s sequence-to-sequence tutorial and you look at the code in `rnn/translate/seq2seq_model.py` (in the TensorFlow models), you will notice a few important differences:\n",
    "\n",
    "- First, so far we have assumed that all input sequences (to the encoder and to the decoder) have a constant length. But obviously sentence lengths may vary:\n",
    "    - It can be handled by using the sequence_length argument to the `static_rnn()` or `dynamic_rnn()` functions\n",
    "    - Sentences are grouped into buckets of similar lengths, and the shorter sentences are padded using a special padding token. \n",
    "        - For example \"I drink milk\" becomes \"<pad> <pad> <pad> milk drink I\", and its translation becomes \"Je bois du lait <eos> <pad>\".\n",
    "\n",
    "- Second, when the output vocabulary is large, outputting a probability for each and every possible word will be slow. Computing the softmax function over such a large vector would be very computationally intensive.\n",
    "    - One solution is to let the decoder output a smaller vectors, then use a sampling technique to estimate the loss without having to compute it over every single word in the target vocabulary. \n",
    "    - Sampled Softmax technique was introduced in 2015 by Sébastien Jean et al.\n",
    "    - In TensorFlow you can use the `sampled_softmax_loss()` function.\n",
    "\n",
    "- Third, the tutorial’s implementation uses an attention mechanism that lets the decoder peek into the input sequence. We will covert it later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Understanding LSTMs\n",
    "    - https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Recurrent Neural Networks w/ TF:\n",
    "    - https://www.tensorflow.org/tutorials/sequences/recurrent\n",
    "    \n",
    "- Keras conv networks:\n",
    "    - https://keras.io/layers/recurrent/\n",
    "    - https://keras.io/applications/\n",
    "    \n",
    "- Deep Learning book\n",
    "    - Ian Goodfellow and Yoshua Bengio and Aaron Courville\n",
    "    - https://www.deeplearningbook.org/contents/rnn.html\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
