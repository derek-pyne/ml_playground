{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To serve the slides: \n",
    "- Open a terminal window \n",
    "- `jupyter nbconvert /full/paht/to/notebook.ipynb --to slides --post serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 3 – Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Outline\n",
    "\n",
    "- Introduction\n",
    "- The convolutional layer\n",
    "    - Filter, stacking, and implementation\n",
    "- Pooling layer\n",
    "- CNN Architectures\n",
    "    - Le Net, AlexNet, GoogLeNet, ResNet\n",
    "    \n",
    "\n",
    "# Learning Objectives\n",
    "\n",
    "- Understanding of:\n",
    "    - Where CNNs came from\n",
    "    - What CNNs building blocks look like\n",
    "    - How to implement them using TensorFlow and Keras. \n",
    "- Exposure to the theory and code to train, and evalute CNNs\n",
    "- Appreciate diverse number of applications with CNNs\n",
    "- Grasp practical considerations: memory, training time, parameters \n",
    "- Review and study the most important CNNs architectures:\n",
    "    - LeNet-5, AlexNet, GoogLeNet, ResNet\n",
    "- Use MNIST as sandbox to understand different levels of abtractions of the CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![Conv. Net.](imgs/convolutional1.png \"Conv Net\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture of the Visual Cortex\n",
    "\n",
    "- David H. Hubel and Torsten Wiesel performed a series of experiments on cats in 1958 and 1959 giving crucial insights on the structure of the visual cortex. (Nobel Prize in Medicine in 1981). \n",
    "- They showed that many neurons in the visual cortex have a small local receptive field:\n",
    "    - The receptive fields of different neurons may overlap, and together they tile the whole visual field. \n",
    "    - They showed that some neurons react only to images of horizontal lines, while others react only to lines with different orientations. \n",
    "    - They also noticed that some neurons have larger receptive fields, and they react to more complex patterns that are combinations of the lower-level patterns. \n",
    "- Their observations led to the idea that the higher-level neurons are based on the outputs of neighboring lower-level neurons\n",
    "\n",
    "![Receptive Fields](imgs/receptivefields.png \"Conv Net\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Not a Fully Connected Network?\n",
    "\n",
    "- This approach works well for small images (e.g., MNIST), however it breaks for larger images due to the large number of parameters required. \n",
    "    - For example, a 100 × 100 image has 10,000 pixels,\n",
    "    - If the first layer has just 1,000 neurons this means a total of 10 million connections,\n",
    "    - And that’s just the first layer!\n",
    "- CNNs solve this problem using partially connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s. \n",
    "\n",
    "- In the last few years, thanks to the increase in computational power, the amount of available training data, and better training technings for deep nets, CNNs have managed to achieve superhuman performance. \n",
    "\n",
    "- They power image search services, self-driving cars, automatic video classification systems, and more. They are also successful at other tasks, such as voice recognition or natural language processing.\n",
    "\n",
    "- Today we will present where CNNs came from, what their building blocks look like, and how to implement them using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "First, let's make sure this notebook has all the required libraries, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#File containing all definitions and utility functions.\n",
    "from setups import *\n",
    "from plotting import *\n",
    "%matplotlib inline\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "- Neurons in the first convolutional layer are not connected to every single pixel in the input image. The neurons are only connected to pixels in their receptive fields.\n",
    "\n",
    "\n",
    "![Conv Fields](imgs/convFirstLayer.png \"First Layer Net\")\n",
    "\n",
    "\n",
    "- In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. \n",
    "\n",
    "\n",
    "- This architecture allows the network to concentrate on low-level features in the first hidden layer, then assemble them into higher-level features in the next hidden layer, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "- A neuron located in row $i$, column $j$ of a given layer is connected to the outputs of the neurons in the previous layer located in rows $i$ to $i + f_h – 1$, columns $j$ to $j + f_w – 1$. \n",
    "\n",
    "\n",
    "### Connections between layers and zero padding\n",
    "\n",
    "\n",
    "- In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs.\n",
    "\n",
    "![ZeroPadding](imgs/convZeroPadding.png \"Zero Padding\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "## Stride\n",
    "\n",
    "- It is also possible to connect a large input layer to a much smaller layer by spacing out the receptive fields. The distance between two consecutive receptive fields is called the stride. \n",
    "\n",
    "\n",
    "\n",
    "##### A 5 × 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of two. \n",
    "\n",
    "![TwoPadding](imgs/convTwoPadding.png \"Two Padding\")\n",
    "\n",
    "\n",
    "- By using a stride greater than one, the dimentionality of the layer can be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filters\n",
    "\n",
    "- A neuron’s weights can be represented as a small image the size of the receptive field. \n",
    "    - Example: Two possible sets of weights, called filters (or convolution kernels). \n",
    "        - The first one is represented as a black square with a vertical white line in the middle. Neurons using these weights will ignore everything in their receptive field except for the central **vertical** line \n",
    "        - The second filter is a black square with a horizontal white line in the middle. Neurons using these weights will ignore everything in their receptive field except for the central **horizontal** line.\n",
    "\n",
    "![ConvFIlters](imgs/convFilters.png \"convFilters\")\n",
    "\n",
    "- A layer full of neurons using the same filter gives you a feature map, which highlights the areas in an image that are most similar to the filter. \n",
    "    - If all neurons in a layer use the same vertical line filter, the layer output  will enhance the white vertical lines, the rest gets blurred. \n",
    "    - Similarly, for the horizontal line filter. \n",
    "- During training, a CNN finds the most useful filters for its task, and it learns to combine them into more complex patterns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacking Multiple Feature Maps\n",
    "\n",
    "- CNN can be composed of several feature maps of equal sizes, so it is more accurately represented in 3D. \n",
    "    - Within one feature map, all neurons share the same parameters \n",
    "    - A convolutional layer simultaneously applies multiple filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.\n",
    "    - Since a feature map shares the same parameters dramatically reduces the number of parameters in the model, but most importantly it means that once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. \n",
    "    - Input images are also composed of multiple sublayers: one per color channel.\n",
    "    \n",
    "![convStackLayers](imgs/convStackLayers.png \"convStackLayers\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacking Multiple Feature Maps\n",
    "\n",
    "- A neuron located in row $i$, column $j$ of the feature map $k$ in a given convolutional layer $l$ is connected to the outputs of the neurons in the previous layer $l – 1$, located in rows $i × s_h$ to $i × s_h + f_h – 1$ and columns $j × s_w$ to $j × s_w + f_w – 1$, across all feature maps (in layer $l – 1$).\n",
    "\n",
    "![image.png](imgs/convEquation.png)\n",
    "\n",
    "- $z_{i,j,k}$ is the output of the neuron located in row $i$, column $j$ in feature map $k$ of the convolutional layer (layer $l$).\n",
    "\n",
    "- $s_h$ and $s_w$ are the vertical and horizontal strides, $f_h$ and $f_w$ are the height and width of the receptive field, and $f_n′$ is the number of feature maps in the previous layer (layer $l – 1$).\n",
    "\n",
    "- $x_{i′, j′, k′}$ is the output of the neuron located in layer $l – 1$, row $i′$, column $j′$, feature map $k′$\n",
    "\n",
    "- $b_k$ is the bias term for feature map $k$ (in layer $l$). \n",
    "\n",
    "- $w_{u, v, k′ ,k}$ is the connection weight between any neuron in feature map $k$ of the layer $l$ and its input located at row $u$, column $v$, and feature map $k′$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "- In TensorFlow, each input image is typically represented as a 3D tensor of shape [height, width, channels]. \n",
    "- A mini-batch is represented as a 4D tensor of shape [mini-batch size, height, width, channels]. \n",
    "- The weights of a convolutional layer are represented as a 4D tensor of shape [fh, fw, fn′, fn]. The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [fn].\n",
    "\n",
    "The following code loads two sample images. Then it creates two 7 × 7 filters, and applies them to both images using a convolutional layer built using TensorFlow’s tf.nn.conv2d() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "dataset = np.array([china, flower], dtype=np.float32)\n",
    "batch_size, height, width, channels = dataset.shape\n",
    "\n",
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "filters[3, :, :, 1] = 1  # horizontal line\n",
    "\n",
    "# Create a graph with input X plus a convolutional layer applying the 2 filters\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "convolution = tf.nn.conv2d(X, filters, strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(convolution, feed_dict={X: dataset})\n",
    "\n",
    "plt.imshow(output[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for image_index in (0, 1):\n",
    "    for feature_map_index in (0, 1):\n",
    "        plot_image(output[image_index, :, :, feature_map_index])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF Padding\n",
    "\n",
    "- Padding must be either \"VALID\" or \"SAME\":\n",
    "\n",
    "    - If set to \"VALID\", the convolutional layer does not use zero padding, and may ignore some rows and columns at the bottom and right of the input image.\n",
    "    - If set to \"SAME\", the convolutional layer uses zero padding if necessary.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "![tfPadding.png](imgs/tfPadding.png)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "- While training CNNs the algorithm will discover the best filters automatically. \n",
    "- `tf.layers.conv2d()` creates the filters (named kernel), and initializes it randomly. It also create the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Creates an input placeholder followed by a convolutional layer \n",
    "# with two 7 × 7 feature maps, using 2 × 2 strides \n",
    "\n",
    "X = tf.placeholder(shape=(None, height, width, channels), dtype=tf.float32)\n",
    "conv = tf.layers.conv2d(X, filters=2, kernel_size=7, strides=[2,2],\n",
    "                        padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### - Convolutional layers have several hyperparameters: \n",
    "    - Number of filters,\n",
    "    - Filter height and width, \n",
    "    - Strides, \n",
    "    - and the padding type. \n",
    "- One can use cross-validation to find the right hyperparameter values, but this is very time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory requirements\n",
    "\n",
    "- Convolutional layers require a huge amount of RAM, especially during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass.\n",
    "\n",
    "Example: A convolutional layer with 5 × 5 filters, outputting 200 feature maps of size 150 × 100, with stride 1 and SAME padding. \n",
    "- If the input is a 150 × 100 RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 \n",
    "- However, each of the 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications. \n",
    "- If the feature maps are represented using 32-bit floats, then the convolutional layer’s output will occupy 200 × 150 × 100 × 32 = 96 million bits (about 11.4 MB) of RAM.\n",
    "- If a training batch contains 100 instances, then this layer will use up over 1 GB of RAM!\n",
    "\n",
    "If training crashes because of an out-of-memory error, try:\n",
    "- reducing the mini-batch size,\n",
    "- reducing dimensionality using a stride, \n",
    "- remove a few layers,\n",
    "- use 16-bit floats instead of 32-bit floats,\n",
    "- distribute the CNN across multiple devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling layer\n",
    "\n",
    "- Their goal is to subsample the input image in order to reduce the computational load, memory usage, and number of parameters\n",
    "- Each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field\n",
    "- Parameters: size, stride, and padding type. \n",
    "- Note that, a pooling neuron has no weights; it aggregates the inputs using a function such as the max or mean. \n",
    "\n",
    "![poolingLayer.png](imgs/poolingLayer.png)\n",
    "\n",
    "In this example, we use a 2 × 2 pooling kernel, a stride of 2, and no padding. max aggregation\n",
    "\n",
    "\n",
    "- A small 2 × 2 kernel and a stride of 2. Will make the output two times smaller in both directions (so its area will be four times smaller).\n",
    "\n",
    "- A pooling layer works on every input channel independently, so the output depth is the same as the input depth. \n",
    "- You may alternatively pool over the depth dimension.\n",
    "\n",
    "The following code creates a max pooling layer using a 2 × 2 kernel, stride 2, and no padding, then applies it to all the images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size, height, width, channels = dataset.shape\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(max_pool, feed_dict={X: dataset})\n",
    "\n",
    "plt.imshow(output[0].astype(np.uint8))  # plot the output for the 1st image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The ksize argument contains the kernel shape along all four dimensions of the input tensor:\n",
    "\n",
    "`              [batch size, height, width, channels]`.\n",
    "\n",
    "- To create an average pooling layer, just use the avg_pool() function instead of max_pool()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN Architectures:\n",
    "\n",
    "- CNN architectures stack a few convolutional layers, then a pooling layer, then another few convolutional layers, then another pooling layer, and so on. \n",
    "- The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper. \n",
    "- At the top of the stack, a regular feedforward neural network is added, and the final layer outputs the prediction.\n",
    "\n",
    "\n",
    "![typicalConvolutional.png](imgs/typicalConvolutional.png)\n",
    "\n",
    "\n",
    "- Variants of this fundamental architecture have been developed, leading to amazing advances in the field. A good measure of this progress is the error rate in competitions such as the ILSVRC ImageNet challenge. \n",
    "\n",
    "- We will first look at the classical LeNet-5 architecture (1998), then three of the winners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LeNet-5\n",
    "\n",
    "It was created by Yann LeCun in 1998 and widely used for handwritten digit recognition (MNIST).\n",
    "\n",
    "![lenet.jpg](imgs/lenet.jpg)\n",
    "\n",
    "\n",
    "- MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and normalized before being fed to the network. \n",
    "\n",
    "- For each neuron in the the pooling layer, it computes the mean of its inputs, then multiplies the result by a learnable coefficient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function.\n",
    "\n",
    "- Most neurons in C3 maps are connected to neurons in only three or four S2 maps \n",
    "\n",
    "- Output layer: each neuron outputs the square of the Euclidian distance between its input vector and its weight vector. Each output measures how much the image belongs to a particular digit class. \n",
    "\n",
    "[Yann LeCun’s website](http://yann.lecun.com/exdb/lenet/index.html) features great demos of LeNet-5 classifying digits.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "- The AlexNet CNN architecture won the 2012 ImageNet ILSVRC challenge:\n",
    "    - It achieved 17% top-5 error rate while the second best achieved only 26%! \n",
    "- It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. \n",
    "- It is quite similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. \n",
    "\n",
    "![alexnet.jpg](imgs/alexnet.png)\n",
    "\n",
    "\n",
    "- To reduce overfitting, the authors used two regularization techniques:\n",
    "    - dropout (with a 50% dropout rate) during training to the outputs of layers F8 and F9. \n",
    "    - They performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n",
    "\n",
    "- AlexNet also uses local response normalization immediately after the ReLU step of layers C1 and C3. This form of normalization makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps.\n",
    "    - This normalization encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization.\n",
    "\n",
    "![eqAlexnet.jpg](imgs/eqAlexnet.png)\n",
    "\n",
    "\n",
    "- $b_i$ is the normalized output of the neuron located in feature map i, at some row u and column v.\n",
    "\n",
    "- $a_i$ is the activation of that neuron after the ReLU step, but before normalization.\n",
    "\n",
    "- $k, \\alpha, \\beta$ and $r$ are hyperparameters. $k$ is the bias, and $r$ is  the depth radius.\n",
    "\n",
    "- $f_n$ is the number of feature maps.\n",
    "\n",
    "- For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation of the neurons located in the feature maps immediately above and below its own.\n",
    "\n",
    "- In AlexNet, the hyperparameters are set as follows: $r = 2$, $\\alpha = 0.00002$, $\\beta = 0.75$, and $k = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GoogLeNet\n",
    "\n",
    "- The GoogLeNet architecture was developed by Christian Szegedy et al. from Google Research.\n",
    "- Increase in performace mainly comes from much deeper CNNs. \n",
    "- GoogLeNet uses sub-networks called inception modules(Think of it as an output feature maps that capture complex patterns at various scales), which allow GoogLeNet to use parameters much more efficiently than previous architectures:\n",
    "    - GoogLeNet actually has 10 times fewer parameters than AlexNet.\n",
    "\n",
    "Inception module. “3 × 3 + 2(S)” means that the layer uses a 3 × 3 kernel, stride 2, and SAME padding.\n",
    "![inceptionmodule.png](imgs/inceptionmodule.png)\n",
    "\n",
    "\n",
    "- The input signal to the inception module is copied and fed to four different layers. \n",
    "- All convolutional layers use the ReLU activation function.\n",
    "- The second set of convolutional layers uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different scales. \n",
    "- Every single layer uses a stride of 1 and SAME padding , so their outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimension in the final depth concat layer \n",
    "\n",
    "- The 1 × 1 kernels, serve two purposes:\n",
    "\n",
    "    - Dimentionality reduction: They are configured to output many fewer feature maps than their inputs, and \n",
    "    - Second, each pair of convolutional layers (`[1 × 1, 3 × 3]` and `[1 × 1, 5 × 5]`) acts like a single, powerful convolutional layer, capable of capturing more complex patterns. \n",
    "\n",
    "- The GoogLeNet CNN includes nine inception modules that actually contain three layers each. \n",
    "- The six numbers in the inception modules represent the number of feature maps output by each convolutional layer in the module. All the convolutional layers use the ReLU activation function.\n",
    "\n",
    "\n",
    "![googlelenet.png](imgs/googlelenet.png)\n",
    "\n",
    "\n",
    "- The first two layers divide the image’s height and width by 4.\n",
    "- Then the local response normalization layer ensures that the previous layers learn a wide variety of features.\n",
    "- Two convolutional layers follow, where the first acts like a bottleneck layer.\n",
    "- Next a max pooling layer reduces the image height and width by 2, \n",
    "- Then comes the tall stack of nine inception modules, interleaved with a couple max pooling layers to reduce dimensionality.\n",
    "- Next, the average pooling layer uses a kernel the size of the feature maps with VALID padding, outputting 1 × 1 feature maps. This makes it unnecessary to have several fully connected layers at the top of the CNN, considerably reducing the number of parameters in the network and limiting the risk of overfitting.\n",
    "- The last layers are: dropout for regularization, then a fully connected layer with a softmax activation function to output estimated class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ResNet\n",
    "\n",
    "- Developed by Kaiming He et al.: Extremely deep CNN composed of 152 layers. \n",
    "- Some of the connections are skipped (also called shortcut connections): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack. \n",
    "\n",
    "- What the residual part? \n",
    "    - When training a neural network, the target function is $h(x)$.\n",
    "    - If we add the input $x$ to the output, then the network will be forced to model $f(x) = h(x) – x$. \n",
    "    This is called residual learning.\n",
    "\n",
    "\n",
    "![reslearning.png](imgs/reslearning.png)\n",
    "\n",
    "\n",
    "\n",
    "- When a neural network is initialized, its weights are close to zero. If we add a skip connection, the resulting network just outputs a copy of its inputs;\n",
    "- If the target function is  close to the identity function, this will speed up training.\n",
    "\n",
    "- With the skip connections, the network can start making progress even if some layers have not started learning yet \n",
    "\n",
    "Deep network vs ResNet\n",
    "![resvsdeep.png](imgs/resvsdeep.png)\n",
    "\n",
    "\n",
    "\n",
    "- The networ starts and ends exactly like GoogLeNet, and in between a very deep stack of  residual units. \n",
    "- Each residual unit is composed of two convolutional layers, with Batch Normalization (BN) and ReLU activation.\n",
    "\n",
    "\n",
    "\n",
    "![googleres.png](imgs/googleres.png)\n",
    "\n",
    "\n",
    "\n",
    "- The number of feature maps is doubled every few residual units, at the same time as their height and width are halved. \n",
    "\n",
    "\n",
    "![skipconnection.png](imgs/skipconnection.png)\n",
    "\n",
    "\n",
    "- ResNet-34 is the ResNet with 34 layers,\n",
    "    - It contains: three residual units that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps.\n",
    "\n",
    "- ResNet-152, use diferent residual units, which have three convolutional layers: \n",
    "    - first a 1 × 1 convolutional layer with just 64 feature maps ,\n",
    "    - then a 3 × 3 layer with 64 feature maps, \n",
    "    - and finally another 1 × 1 convolutional layer with 256 feature maps\n",
    "    - ResNet-152 contains three such RUs that output 256 maps, then 8 RUs with 512 maps, a  36 RUs with 1024 maps, and finally 3 RUs with 2,048 maps.\n",
    "\n",
    "- Other architectures to consider: \n",
    "    - VGGNet13 (runner-up of the ILSVRC 2014 challenge) \n",
    "    - Inception-v414 (which merges the ideas of GoogLeNet and ResNet and achieves close to 3% top-5 error rate on ImageNet classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST Example\n",
    "\n",
    "Note: instead of using the `fully_connected()`, `conv2d()` and `dropout()` functions from the `tensorflow.contrib.layers` module (as in the book), we now use the `dense()`, `conv2d()` and `dropout()` functions (respectively) from the `tf.layers` module, which did not exist when this chapter was written. This is preferable because anything in contrib may change or be deleted without notice, while `tf.layers` is part of the official API. As you will see, the code is mostly the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "For all these functions:\n",
    "* the `scope` parameter was renamed to `name`, and the `_fn` suffix was removed in all the parameters that had it (for example the `activation_fn` parameter was renamed to `activation`).\n",
    "\n",
    "The other main differences in `tf.layers.dense()` are:\n",
    "* the `weights` parameter was renamed to `kernel` (and the weights variable is now named `\"kernel\"` rather than `\"weights\"`),\n",
    "* the default activation is `None` instead of `tf.nn.relu`\n",
    "\n",
    "The other main differences in `tf.layers.conv2d()` are:\n",
    "* the `num_outputs` parameter was renamed to `filters`,\n",
    "* the `stride` parameter was renamed to `strides`,\n",
    "* the default `activation` is now `None` instead of `tf.nn.relu`.\n",
    "\n",
    "The other main differences in `tf.layers.dropout()` are:\n",
    "* it takes the dropout rate (`rate`) rather than the keep probability (`keep_prob`). Of course, `rate == 1 - keep_prob`,\n",
    "* the `is_training` parameters was renamed to `training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN on Keras\n",
    "\n",
    "\n",
    "Follow tutorial :\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000,28,28,1)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Build the network:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Convolutional Neural Networks w/ TF:\n",
    "    - https://www.tensorflow.org/tutorials/images/deep_cnn\n",
    "\n",
    "- Build a Convolutional Neural Network using Estimators\n",
    "    - https://www.tensorflow.org/tutorials/estimators/cnn\n",
    "    \n",
    "- Keras conv networks:\n",
    "    - https://keras.io/layers/convolutional/\n",
    "    - https://keras.io/applications/\n",
    "    \n",
    "- Deep Learning book\n",
    "    - Ian Goodfellow and Yoshua Bengio and Aaron Courville\n",
    "    - https://www.deeplearningbook.org/contents/convnets.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
