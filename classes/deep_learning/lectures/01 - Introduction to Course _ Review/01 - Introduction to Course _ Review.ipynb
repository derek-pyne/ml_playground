{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCS 3546 Week 1 - Introduction to Course & Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Introduction to the course\n",
    " - Course outline\n",
    " - Meet other learners\n",
    "- Introduction to Deep Learning\n",
    " - Applications of Deep Learning\n",
    " - Moral issues\n",
    " - Review of key Machine Learning concepts\n",
    " - Introduction to Information Theory\n",
    " - Getting started with Google Colab and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Develop familiarity with course and university logistics to prepare for success\n",
    "- Discuss as a group some of the moral and ethical issues of AI\n",
    "- Identify some existing applications of Deep Learning and enable you to discover others\n",
    "- Review material from 3253 required for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the course\n",
    "Welcome to SCS 3546 Deep Learning, the second course in the Certificate in AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Certificates in Data Science\n",
    "The University of Toronto School of Continuing Studies offers three Data Science-related certificates:\n",
    "- Management of Enterprise Analytics: This certificate is for people who will be managing or working with Data Science teams and need an understanding of the key management issues such as privacy and security and a survey of the core data science techniques.\n",
    "- Data Science: This certificate covers the key statistical methods of Data Science, the technologies for analyzing small and massive distributed datasets, and the inner workings of machine learning algorithms, all using hands-on tools and exercises in Python.  The certificate includes four courses: Foundations of Data Science, Statistics for Data Science, Big Data Tools and Machine Learning.\n",
    "- Artificial Intelligence: This certificate shares the Machine Learning course from the Data Science certificate and adds two additional courses, Deep Learning (this course), which extends the introduction to Deep Learning and TensorFlow from the Machine Learning course into specific Deep Learning architectures and their applications; and Intelligent Agents & Reinforcement Learning which covers search and learning techniques for problems (such as game-playing) where there isn't a single best strategy but some moves are likely better than others and there is occasional feedback that the strategy seems to be working well or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course outline\n",
    "The course outline is provided in a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introductions\n",
    "One of the objectives of courses in our certificate programs is to provide an opportunity for you to widen your network of industry contacts.  Please introduce yourself to the class and tell us a little about yourself:\n",
    "- Who you work for\n",
    "- Your role\n",
    "- Previous experience with Python, TensorFlow and Deep Learning\n",
    "- Why you're taking the course and what you'd like to get out of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health, safety and local services\n",
    "- Fire exits\n",
    "- Washrooms\n",
    "- Coffee\n",
    "- Parking\n",
    "- TCards and Library Access: TCard offices are listed at http://tcard.utoronto.ca/contact-us/\n",
    "- U of T Closures: https://onesearch.library.utoronto.ca/holiday-hours-and-closures https://www.utoronto.ca/campus-status\n",
    "- If coming by TTC for a weekend class be sure to check TTC status in advance https://www.ttc.ca/Service_Advisories/index.jsp\n",
    "- Break: About half way through for 10-15 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News of the Week\n",
    "The instructor may ask for volunteers (one per week) to prepare a 5 minute presentation on current events in the world of neural nets at the beginning of each class.  Learners who are in class on time will catch the latest news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get the most out of the course\n",
    "- Do the readings\n",
    "- Be on time\n",
    "- Work/share with/help other attendees\n",
    "- Do the assignments\n",
    "- If you get stuck\n",
    "  1. Try googling the error message: you'll probably find something useful\n",
    "  2. Post a question on the portal: the instructor and another learners will help\n",
    "  3. If those don't work, email the instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning outcomes\n",
    "Be able to:\n",
    "- Describe the current applications of Deep Learning and propose new applications\n",
    "- Recognize and discuss the moral issues of AI\n",
    "- Recall key concepts from the Machine Learning course that you will need for this course\n",
    "- Use Information Theory to reason about the quality of a model\n",
    "- Launch and begin to use TensorFlow to run Deep Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Deep Learning\n",
    "There is a wide variety of applications of Deep Learning and more appearing all the time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Image recognition\n",
    "- Facial recognition\n",
    "  - Friends on Facebook: Facebook uses image recognition to identify people in photos\n",
    "  - Your photos from a competition: There are services now that will, for example, find all the posted or intentional photos taken by people along a marathon route you've run that have you in them\n",
    "- Seeing AI: This is an experimental phone app that attempts to identify and describe objects in photos you take (intended as a near-real-time aid for the visually impaired) https://www.microsoft.com/en-us/seeing-ai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Voice recognition and intelligent assistants\n",
    "\n",
    "- Siri, Google Home, Amazon Alexa, etc.: These services use deep nets to recognize spoken words and extract meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Medical diagnostics and drug design\n",
    "- X-ray and image analysis: Deep nets now perform at levels similar to radiologists in differentiating diseased from normal images.  This will allow radiologists to focus on the complex cases and avoid spending time on images that are normal with high confidence.\n",
    "- Medical CAD/CAM: Machine and Deep Learning are finding applications in systems for reconstructing damaged bones and joints.\n",
    "- ECG analysis: Deep Learning is beginning to be used to analyze cardiograms for signs of disease.\n",
    "- Eye and skin disease identification: It's also being used to analyze images of retinas to track progress of diseases such as glaucoma and skin blemishes that may have potentially cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Autonomous vehicles and factories\n",
    "- University of Toronto / Uber Advanced Technologies Group: Uber's self-driving car team is located at the University of Toronto https://www.utoronto.ca/news/autonomous-vehicles-u-t-researchers-make-advances-new-algorithm\n",
    "- Boeing: Boeing has a website specific to their autonomous systems for military and commercial purposes https://www.boeing.com/defense/autonomous-systems/index.page\n",
    "- Autonom driverless taxi: http://navya.tech/autonom/cab\n",
    "- Power generation: https://www.bloomberg.com/news/articles/2018-04-09/forget-cars-mitsubishi-hitachi-sees-autonomous-power-plants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Language translation\n",
    "- Google Translate: Deep nets were added to Google Translate in 2016.  It translates entire sentences at a time rather than individual words so the translation uses the context in which each word appears to produce a better result than if the words were translated individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Playing board games\n",
    "- Particularly in combination with Reinforcement Learning (Deep Reinforcement Learning)\n",
    "- DeepMind AlphaGo beat the world's best Go player 4-1 in a five-game match in 2016: https://deepmind.com/research/alphago/.  Go is considered a much more difficult game than chess as the number of possible moves on each turn is far larger and the ramifications of each move more difficult to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 Artistic and entertainment applications\n",
    "- Style transfer: These nets take a style from one image such as a painting and apply it to another image.\n",
    "  - Lucid: https://github.com/tensorflow/lucid\n",
    "  - Prisma: https://prisma-ai.com/\n",
    "- Music and sound synthesis\n",
    "  - Google Project Magenta: https://magenta.tensorflow.org/\n",
    "- Repairing or adding detail to images:\n",
    "  - https://www.nvidia.com/research/inpainting/\n",
    "  - https://www.resetera.com/threads/ai-neural-networks-being-used-to-generate-hq-textures-for-older-games-you-can-do-it-yourself.88272/\n",
    "  - https://arxiv.org/abs/1609.04802"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moral issues of AI\n",
    "The World Economic Forum identified the Top 9 Ethical Issues of AI (https://www.weforum.org/agenda/2016/10/top-10-ethical-issues-in-artificial-intelligence/)\n",
    "1. Unemployment: What happens after the end of jobs?\n",
    "2. Inequality: How do we distribute the wealth created by machines?\n",
    "3. Humanity: How do machines affect our behaviour and interaction?\n",
    "4. Artificial stupidity: How can we guard against mistakes?\n",
    "5. Racist robots: How do we eliminate AI bias?\n",
    "6. Security: How do we keep AI safe from adversaries?\n",
    "7. Evil genies: How do we protect against unintended consequences?\n",
    "8. Singularity: How do we stay in control of a complex intelligent system?\n",
    "9. Robot rights: How do we define the humane treatment of AI?\n",
    "\n",
    "Class Discussion:\n",
    "- Should a self-driving car sacrifice its owner to save a pedestrian?\n",
    "- Should an app tell you that you likely have a serious disease?\n",
    "- Who is liable if an AI makes a bad call?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of key concepts from previous courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters & Hyperparameters\n",
    "- Supervised Learning involves finding the set of model parameter values that cause a model (such as a neural net) to best fit an observed input-to-output mapping\n",
    "- Modern neural nets have thousands or millions of parameters referred to as weights and biases\n",
    "- We call the process of determining the settings of the parameters that cause the model to produce results similar to a dataset of inputs and corresponding outputs \"learning a model\"\n",
    "- Some parameters can't be learned (either in principle or because we don't know how to yet).  For example: how many layers a neural net should have.  We call these parameters, that must be set by the data scientist \"hyperparameters\".\n",
    "- Hyperparameters sometimes become parameters as our body of knowledge improves and theory provides guidance for optimum (or at least good) settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capacity & Overfitting\n",
    "- The more parameters a model has, the more flexible it is to fit the observed input to output mapping.  For example, think of mapping a single variable to a single variable by using a line of best fit versus a polynomial.  As we add more parameters (terms and therefore degrees to the polynomial) it becomes more flexible and can approximate more and more complicated functions.\n",
    "- If one model is more flexible than another we say it has higher \"capacity\".\n",
    "- But higher capacity isn't necessary good.  We want our models to generalize.  If the actual relationship in the training data is linear (plus noise) and we use a higher-order polynomial, the curve will start to fit to the noise in the data and make increasingly noisy predictions.  We want the model to learn the overall general patterns in the data, not the details of any particular dataset and its unique quirks.\n",
    "- If a model is too flexible (has too many parameters) it can \"overfit\".  Overfitting happens when the model \"rote learns\" rather than extracting generalities.  The learning process will cause the model to contort the prediction function however necessary to make it pass near the observed points; between those points the function can end up far from the observed values and hence make for terrible predictions.\n",
    "- This is usually because the parameters (weights) have taken on large values.\n",
    "- Techniques called \"normalization\" methods exist for constraining the weights from growing too large, helping a model from becoming overfit.  (Note that the word \"normalization\" has many different meanings in statistics so be careful to understand what a so-called normalization feature in a library actually does before using it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial Neural Nets\n",
    "- A neural net is a layered network of decision-making or curve-fitting units called neurons.\n",
    "- Although the idea of the first artificial neural nets was inspired by brain research in the late 1800's and early 1900's, the operation of brain cells is much more complex than artificial neurons so we must be careful not to oversell the similarity.\n",
    "- An artificial neural net can approximate (closely) any continuous function if it has sufficient capacity (number of neurons).  This is called \"universality\".\n",
    "- Each layer of a neural net is almost always fully connected to the layer before it and sometimes there are connections between non-adjacent layers.\n",
    "- Layers other than the input and output layer are called \"hidden\" layers (but there isn't really any other significance to the term).\n",
    "- \"Deep\" means that the network has more than one hidden layer.\n",
    "- Training or Learning for a neural net means finding a weight and bias for each neuron that causes the net to best mimic (avoiding overfitting) the input-to-output mapping implied by the dataset used for the training.\n",
    "- To begin with we typically initialize a neural net with small random weights to help the training process get started.\n",
    "- An important word of caution: We need to be careful when using a trained network where the net might see inputs in production use that it wasn't exposed to during training; in this case the behaviour of the net could be completely unpredictable: in effect it's being asked a question it's never seen anything like before and the response could be completely inappropriate.  Safeguards against nonsense outputs would be prudent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions\n",
    "- In order for a net to learn a mapping, it needs to know how to adjust its weights and biases to fit the training set.  As it is learning, it needs some measure of how accurate its current predictions are and in which direction to adjust its parameters in order to improve its predictions.\n",
    "- There are several well-known alternative measures of how far a model is from being optimum.  These measures are called loss or error functions.\n",
    "- For data with a Gaussian distribution the theoretically best measure is the average squared error (i.e. the sum of the squares of the differences between each prediction and the known value, divided by the number of observations). This is what Ordinary Least Squares Regression uses but this measure is not usually the best for training neural nets.\n",
    "- We will see several alternative loss functions and their relative merits in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "- Some loss functions, such as quadratic loss (i.e. the average squared error above), can be precisely optimized in a known number of operations with a specific algorithm.  For example, most stats packages solve for a line of best fit using a standard linear algebra method that is guaranteed to produce a unique optimum in a predictable amount of time.\n",
    "- For other loss functions there is no known method guaranteed to converge to a minimum.\n",
    "- In these cases we use a technique known as gradient descent where the algorithm follows the gradient (slope) of the loss function downwards towards a minimum.  If you think of the case where there are two parameters to be learned, the loss function is a surface that we're trying to find the lowest point of.  If the loss is quadratic it's shaped like a bowl and there's a clear minimum.  But it it's more complex it may many local minima.  Algorithms can only test one point on the surface at a time; they can't see the entire surface at a glance and see where the global minimum is.\n",
    "- Fortunately however for most applications a good minimum is \"good enough\" even if it isn't the ultimate minimum.\n",
    "- Calculating the loss and gradient precisely at a point (the current settings of all the parameters) is expensive because it is a function of *all* the data in the training set and training a neural net takes *a lot* of data\n",
    "- Learning can be accelerated significantly by using an approximation of the gradient calculated by using a sampled subset of the training data\n",
    "- This is called *stochastic* gradient descent because of the random sampling of the data at each learning step\n",
    "- Each period in which all of the data has been used is called an *epoch*: for example, if 1/10 of the data is sampled at each step, there are 10 steps in an epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Information Theory?\n",
    "- Branch of applied mathematics concerned with quantifying the information content of a noisy signal\n",
    "- Originated with Claude Shannon at Bell Labs in 1948\n",
    "- Set the theoretical basis for reliable digital communications and data compression\n",
    "- Introduced the key concept of \"Entropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "- Key measure of Information Theory\n",
    "- Quantifies the amount of uncertainty involved in a random variable\n",
    "- Consider:\n",
    " - Life on an island where it rarely rains\n",
    " - If someone told you it was not going to rain tomorrow would that be useful/valuable/surprising to you?\n",
    " - What if they said they knew for sure it would (and are trustworthy)?\n",
    "- Entropy measures the unexpectedness of an outcome\n",
    "- Shannon Entropy H (in bits per symbol) is defined as $H = -\\sum_{i} p_i \\log_2(p_i)$\n",
    "- More specifically, Entropy is the average amount of information you learn from an outcome\n",
    "- In our island example:\n",
    " - Info from \"going to rain\": $I = -\\log_2(1/32) = 5\\ bits$ (see next cell)\n",
    " - Info from \"not going to rain\": $I = -\\log_2(31/32) = 0.046\\ bits$\n",
    "- $H = (1/32)* 5 + (31/32) * 0.46 = 0.20\\ bits$\n",
    "- We get 0.2 bits of information on average over many weather reports\n",
    "- H can also be expressed in units called \"nats\": use the natural $\\log_e$ rather than $\\log_2$\n",
    "- The ideas of information theory easily extend to continuous variables which are more common in machine learning\n",
    "- Watch this video: https://www.youtube.com/watch?v=ErfnhcEV1O8&t=7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing log base 2 and Entropy (H)\n",
    "import math\n",
    "\n",
    "print('The -log base 2 of 1/32 is:', -math.log(1.0/32.0, 2))\n",
    "print('The -log base 2 of 31/32 is:', -math.log(31.0/32.0, 2))\n",
    "print('H is:', 1/32 * 5 + 31/32 * 0.046)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: Compute H if the chance or rain on any day is 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL Divergence\n",
    "- *Kullback-Leibler divergence* (also known as relative entropy) is a measure of how different one distribution is from another\n",
    "- If we compared a distribution to itself it would have a KL divergence of 0\n",
    "- You can think of it as being like a distance between two distributions: it's always positive and the more different the distributions, the higher the number\n",
    "- But it isn't exactly a distance: the KL divergence between distributions *p* and *q* is different than the KL divergence between *q* and *p*\n",
    "- KL divergence can also be thought of as the number of additional bits required to encode samples from *p* using a code optimized for *q*\n",
    "- It can also be thought of in Bayesian terms as the information gained when one revises one's beliefs from the prior probability distribution *q* to the posterior *p*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL According to @SimonDeDeo May 8, 2018 Twitter\n",
    "Kullback-Leibler divergence has an enormous number of interpretations and uses:\n",
    "- Psychological: an excellent predictor of where attention is directed\n",
    "- Epistemic: a normative measure of where you ought to direct your experimental efforts (maximize expected model-breaking) http://www.jstor.org/stable/4623265 \n",
    "- Thermodynamic: a measure of work you can extract from an out-of-equlibrium system as it relaxes to equilibrium\n",
    "- Statistical: too many to count, but for example, a measure of the failure of an approximation method https://t.co/h4L0O2VZXa\n",
    "- Computational (machine learning): a measure of model inefficiencyâ€”the extent to which it retains useless information\n",
    "- Computational (compression): the extent to which a compression algorithm designed for one system fails when applied to another\n",
    "- And more (geometrical, biological, etc.; do a Twitter search to check out the whole thread)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy\n",
    "- The cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an \"artificial\" probability distribution q, rather than the \"true\" distribution p. https://en.wikipedia.org/wiki/Cross_entropy\n",
    "- Average message length if the message was binary-encoded\n",
    "- Is equal to the Entropy of p + the KL divergence between p and q:\n",
    "\n",
    "    $H(p,q) =  H_p + D_{KL} (p||q)$\n",
    "- If you knew how to encode the information perfectly, the cross-entropy would just be equal to the entropy; if your model is off, it will be off by an amount equal to the KL divergence, which we need to add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimation\n",
    "- Say we have a probabilistic model that has some parameters that determine its shape\n",
    "- A simple example would be the Gaussian distribution which has two parameters\n",
    "- Neural nets can have thousands or millions of parameters (usually called weights)\n",
    "- Given a dataset of observed inputs and a resulting output and a parametric model we would like to find the parameters that would make the model make predictions for each input as similar as possible to the actual observed output (leaving aside overfitting for a moment)\n",
    "- Maximum Likelihood Estimation is a mathematical method for finding the values of the parameters that result in this \"best fit\"\n",
    "- In effect MLE minimizes the dissimilarity between the observed distribution of the training set and the model distibution (using KL Divergence or cross-entropy as the measure of dissimilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Installing TensorFlow\n",
    "- Installing TensorFlow is a bit tricky and the instructions vary with each new release.  See https://www.tensorflow.org/install/ for the latest installation instructions.\n",
    "- Note: Unfortunately TensorFlow no longer supports using the GPU on Mac's (but you can do all the assignments in this course without GPU support, training the net will just take longer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a cloud account\n",
    "- We will be using Google Colaboratory: https://colab.research.google.com\n",
    "- Other Alternatives\n",
    "  - Google Cloud Platform: https://cloud.google.com/solutions/running-distributed-tensorflow-on-compute-engine\n",
    "  - Microsoft Azure: https://blogs.msdn.microsoft.com/uk_faculty_connection/2017/03/27/azure-gpu-tensorflow-step-by-step-setup/\n",
    "  - AWS: https://aws.amazon.com/machine-learning/amis/\n",
    "  - Crestle, PaperSpace (see Resources section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensors\n",
    "\n",
    "- Tensors for our purposes are simply multi-dimensional arrays (like NumPy ndarrays)\n",
    "- The number of dimensions is referred to as the rank\n",
    " - Scalars are rank-0\n",
    " - Vectors are rank-1\n",
    " - Ordinary matrices are rank-2\n",
    "- An important question we will face is how to best encode the relevant features of a problem into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hands on\n",
    "# But first: pip install tensorflow  --or-- conda install -c conda-forge tensorflow \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.InteractiveSession() # Make TF execute eagerly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.constant(6.3) # Create a rank-0 tensor (constant scalar) with single-precision floating point value 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros(3) # Create a rank-1 tensor (vector) of length 3 initialized to all zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.zeros((4, 4)).eval() # Note we get an ordinary NumPy array back when we evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why was it tf.zeroes((4,4)) in the last example and not tf.zeroes(4,4)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.ones((2, 3, 3)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.fill((2, 2), value='hello').eval() # Fill every element with a value\n",
    "                                      # (usually a real number but can be other NumPy types e.g. string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.eye(5).eval() # Identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: Create a vector array([1, 2, 3]) using Numpy arange and tf.constant()\n",
    "import numpy as np\n",
    "# Insert your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now use np.arange to create a tensor like this:\n",
    "# array([1, 2, 3],\n",
    "#       [4, 5, 6],\n",
    "#       [7, 8, 9])\n",
    "# Hint: See https://www.tensorflow.org/api_docs/python/tf/constant\n",
    "# Insert your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: Use tf.transpose (see https://www.tensorflow.org/api_docs/python/tf/transpose) to transpose your tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computations in TensorFlow are represented as an instance of a tf.graph object\n",
    "- The graph consists of tf.Tensor objects that hold data and tf.Operation objects that describe mathematical operations on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras is a high-level API that makes TensorFlow easier to use: https://www.tensorflow.org/guide/keras\n",
    "- Keras can be used as a common language for working with other Deep Learning libraries such as Theano (but there may be small variations depending which engine you're using)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras makes it easy to assemble nets layer-by-layer\n",
    "# Here is an example from the TensorFlow introduction to Keras:\n",
    "model = keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# Add another:\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure the model to run with an optimizer\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then add some data and run the learning process:\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- Style transfer\n",
    " - https://medium.com/data-science-group-iitr/artistic-style-transfer-with-convolutional-neural-network-7ce2476039fd\n",
    "- Universality\n",
    " - http://neuralnetworksanddeeplearning.com/chap4.html\n",
    "- Information Theory\n",
    " - https://en.wikipedia.org/wiki/Information_theory\n",
    " - http://www.inference.org.uk/itprnn/book.html\n",
    "- Jupyter notebook\n",
    " - https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "- Seeing AI\n",
    " - https://www.microsoft.com/en-us/seeing-ai\n",
    "- Cloud GPU services\n",
    "  - https://crestle.com\n",
    "  - https://paperspace.com\n",
    "- Keras\n",
    "  - https://www.tensorflow.org/guide/keras\n",
    "- Alpha Go\n",
    "  - https://www.alphagomovie.com/\n",
    "  - https://deepmind.com/research/alphago/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next week\n",
    "Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
