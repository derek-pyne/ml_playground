{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment for Module 5, Training Models\n",
    "\n",
    "In this assignment you will train different models on a given data set, and find the one that performs best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data for the assignment (similar to the notebook from chapter 2 of Hands-On...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the categories in the categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'<1H OCEAN':'LESS_1H_OCEAN', 'INLAND':'INLAND', 'ISLAND':'ISLAND', 'NEAR BAY':'NEAR_BAY', 'NEAR OCEAN':'NEAR_OCEAN'}\n",
    "housing['ocean_proximity'] = housing['ocean_proximity'].map(lambda s: d[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 2 more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy variables based on the categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(housing['ocean_proximity'])\n",
    "housing = housing.drop('ocean_proximity', axis=1)\n",
    "housing = housing.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 16 columns):\n",
      "longitude                   20640 non-null float64\n",
      "latitude                    20640 non-null float64\n",
      "housing_median_age          20640 non-null float64\n",
      "total_rooms                 20640 non-null float64\n",
      "total_bedrooms              20640 non-null float64\n",
      "population                  20640 non-null float64\n",
      "households                  20640 non-null float64\n",
      "median_income               20640 non-null float64\n",
      "median_house_value          20640 non-null float64\n",
      "rooms_per_household         20640 non-null float64\n",
      "population_per_household    20640 non-null float64\n",
      "INLAND                      20640 non-null uint8\n",
      "ISLAND                      20640 non-null uint8\n",
      "LESS_1H_OCEAN               20640 non-null uint8\n",
      "NEAR_BAY                    20640 non-null uint8\n",
      "NEAR_OCEAN                  20640 non-null uint8\n",
      "dtypes: float64(11), uint8(5)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Partition into train and test\n",
    "\n",
    "Use train_test_split from sklearn.model_selection to partition the dataset into 70% for training and 30% for testing.\n",
    "\n",
    "You can use the 70% for training set as both training and validation by using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'median_house_value'\n",
    "features = list(train_set.columns)\n",
    "features = [f for f in features if f!=target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr = train_set[features]\n",
    "y_tr = train_set[[target]]\n",
    "\n",
    "X_te = test_set[features]\n",
    "y_te = test_set[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polynomial transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PolynomialFeatures from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_tr)\n",
    "X_tr = poly.transform(X_tr)\n",
    "X_te = poly.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You should obtain X_tr and X_te with 136 columns each, since originally you had 15 features.\n",
    "\n",
    "##### With m original features, the new added polynomial features of degree 2 are: $(m^2-m)/2+m+1$. Why?\n",
    "\n",
    "##### These, plus the original features gives a total of  $(m^2-m)/2+2m+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 15\n",
      "Final number of features: 136\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of features: \"+str(len(features)))\n",
    "print(\"Final number of features: \"+str(X_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, use StandardScaler from sklearn.preprocessing to normalize the training and testing data, using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear regression on original features (no transformations) --- benchmark\n",
    "\n",
    "#### Your goal is to find the model that minimizes the rmse score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 70549.62536236  69522.44448292  66640.43944345  68700.55956307]\n",
      "Mean: 68853.2672129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_scores = cross_val_score(LinearRegression(), train_set[features], train_set[target], scoring=\"neg_mean_squared_error\", cv=4)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Linear regression  (on transformed features: polynomial transformation + scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do as in 4 but with the original and transformed features (136 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [  2.28961612e+14   7.57419426e+05   6.09181669e+04   1.82025050e+15]\n",
      "Mean: 5.12303027274e+14\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(LinearRegression(), X_tr, y_tr, scoring=\"neg_mean_squared_error\", cv=4)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the error on the cross-validation is too high it is because the model is over-fitting. Regularization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 1000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "param_grid = [{'alpha': [0.001,0.01,0.1,1,10,100,1000,1000]}]\n",
    "grid_search_rr = GridSearchCV(Ridge(), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_rr.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1000}\n",
      "67147.1090497\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_rr.best_params_)\n",
    "print(np.sqrt(-grid_search_rr.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Lasso regression\n",
    "\n",
    "Now do the same as in 6 but with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1000}\n",
      "66596.6539789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "param_grid = [{'alpha': [0.001,0.01,0.1,1,10,100,1000,1000]}]\n",
    "grid_search_lr = GridSearchCV(Lasso(max_iter=5000), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_lr.fit(X_tr, y_tr)\n",
    "print(grid_search_lr.best_params_)\n",
    "print(np.sqrt(-grid_search_lr.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Elastic Net regression\n",
    "\n",
    "Do the same as in 6 and 7, but now with Elastic Net. However, the grid search should be over the parameters alpha and  l 1ratio. Use just 3 values for l1_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'l1_ratio': 0.3}\n",
      "66992.0956835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "param_grid = [{'alpha': [0.001,0.01,0.1,1,10,100,1000,1000], 'l1_ratio': [.3, .5, .7]}]\n",
    "grid_search_en = GridSearchCV(ElasticNet(max_iter=5000), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_en.fit(X_tr, y_tr)\n",
    "print(grid_search_en.best_params_)\n",
    "print(np.sqrt(-grid_search_en.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating your best model on TESTING data\n",
    "\n",
    "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66590.3677763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "final_model = grid_search_en.best_estimator_   ## grid_search SHOULD BE THE BEST GRID SEARCH ##\n",
    "\n",
    "y_te_estimation = final_model.predict(X_te)\n",
    "\n",
    "final_mse = mean_squared_error(y_te, y_te_estimation)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train score: -4252436709.77\n",
      "Mean test score: -4487940884.06\n"
     ]
    }
   ],
   "source": [
    "print('Mean train score: %s' % grid_search_en.cv_results_['mean_train_score'][grid_search_en.best_index_])\n",
    "print('Mean test score: %s' % grid_search_en.cv_results_['mean_test_score'][grid_search_en.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAD8CAYAAAA45tAbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4f8DG0yyF2q1x3wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f915f0bdf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x=y_te['median_house_value'].values, y=y_te_estimation)\n",
    "plt.xlim([-200000,800000])\n",
    "plt.ylim([-200000,800000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Before you computed the final_rmse on the test data, what was your expected value for this quantity? Does your best model have high variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training and testing scores for our best fit, both sets of error are within 10% of each other. This means that during cross-validation, our model was performing similarily on the training and test folds and not showing signs of high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Why does the matrix X appears transponsed in the normal equation in the linear regression? Equation 4.4. Start from equation 4.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equation minimizes the MSE error which means minimizing each of these terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{i=1}^m(\\theta^T\\cdot x^{(i)}-y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize this we take the derivative of the MSE function and set it to 0. This leads us to take the derivative of the equation above with respect to $\\theta$ to find the $\\theta$ that minimizes the equation. However, the dot product of $\\theta^T\\cdot x^{(i)}$ rotates the vector $x^{(i)}$ from a column to a row so when we take the derivative of this sum and aggregate the results back into matrix form, we have rotated $X$ to $X^T$ by this dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, this depends on the shape of the cost function and the learning rate. Not all cost functions are guaranteed to be universally convex, meaning that gradient descent could get stuck in a local minimum. Also, if the learning rate is chosen too high, then even with a convex cost function, the system may be unstable and oscillate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessarily. If the dollar cost for each execution run is very high then stopping immediately when the validation error goes up might be necessary but in most cases it is better to let the training run a little bit longer to ensure that the validation error really is moving up. This is because with small batches, and the random nature of some models, some noise in the performance metrics is expected and it is best to train a little bit longer so that we can be confident that the increase in validation error is signal and not noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter α or reduce it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both training and validation errors are high it likely means our model does not have the capability to model the data (or that there is not really a signal in our data, but we'll ignore that for now). This means the model has high bias and we should try our model on a set of reduced regularization parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
